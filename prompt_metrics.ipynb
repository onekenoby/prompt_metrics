{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03aa9bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stefa\\anaconda3\\envs\\py10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Weak Prompt Summary:\n",
      "Here are a few things about the text:\n",
      "\n",
      "*   It describes a **new AI model** developed by TechLabs researchers.\n",
      "*   The model's main achievement is a **30% reduction in energy consumption** in data centers.\n",
      "*   It uses **reinforcement learning** to manage server workloads efficiently.\n",
      "*   This innovation is expected to lead to **significant cost savings** (millions) and a **positive environmental impact**.\n",
      "\n",
      "In short, the text is about a promising AI technology that dramatically cuts energy use in data centers, offering major economic and environmental benefits.\n",
      "\n",
      "  ROUGE-L: 0.2\n",
      "  BLEU: 0.01\n",
      "  BERTScore-F1: 0.87\n",
      "  Length: 114\n",
      "\n",
      "ðŸ“ Strong Prompt Summary:\n",
      "Researchers have developed a new AI model that uses reinforcement learning to intelligently manage server workloads, achieving a remarkable 30% reduction in data center energy consumption. This breakthrough innovation promises significant cost savings and a major positive impact on the environment by lowering the energy footprint of these massive facilities.\n",
      "\n",
      "  ROUGE-L: 0.24\n",
      "  BLEU: 0.05\n",
      "  BERTScore-F1: 0.91\n",
      "  Length: 53\n",
      "\n",
      "ðŸ† Weighted Score (Weak): 1.722\n",
      "ðŸ† Weighted Score (Strong): 1.85\n",
      "\n",
      "âœ… Strong prompt performed better across the majority of metrics.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "\n",
    "# Ensure punkt tokenizer is available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "# Load .env and configure Gemini\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash-preview-04-17\")\n",
    "\n",
    "# Input and prompts\n",
    "input_text = \"\"\"\n",
    "A groundbreaking AI model developed by researchers at TechLabs has achieved a 30% reduction in \n",
    "energy consumption in data centers.\n",
    "The model uses reinforcement learning to dynamically manage server workloads, avoiding unnecessary energy usage.\n",
    "This could lead to millions in savings and a significant reduction in environmental impact.\n",
    "\"\"\"\n",
    "reference_summary = (\n",
    "    \"TechLabs' new AI model reduces data center energy use by 30% using reinforcement learning to optimize workloads.\"\n",
    ")\n",
    "\n",
    "prompt_weak = \"Say to me something about the text.\"\n",
    "prompt_strong = (\n",
    "    \"You are a technical journalist. In 1â€“2 sentences, summarize the key innovation and its real-world impact for a general audience.\"\n",
    ")\n",
    "\n",
    "def get_summary(prompt, text):\n",
    "    full_prompt = f\"{prompt}\\n\\n{text}\"\n",
    "    response = model.generate_content(full_prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "summary_weak = get_summary(prompt_weak, input_text)\n",
    "summary_strong = get_summary(prompt_strong, input_text)\n",
    "\n",
    "def evaluate_metrics(summary, reference):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge = scorer.score(reference, summary)['rougeL'].fmeasure\n",
    "    ref_tokens = [word_tokenize(reference, preserve_line=True)]\n",
    "    summary_tokens = word_tokenize(summary, preserve_line=True)\n",
    "    bleu = sentence_bleu(ref_tokens, summary_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "    P, R, F1 = bert_score([summary], [reference], lang=\"en\", verbose=False)\n",
    "    bert_f1 = F1[0].item()\n",
    "    length = len(summary_tokens)\n",
    "    return {\n",
    "        'ROUGE-L': round(rouge, 2),\n",
    "        'BLEU': round(bleu, 2),\n",
    "        'BERTScore-F1': round(bert_f1, 2),\n",
    "        'Length': length\n",
    "    }\n",
    "\n",
    "def evaluate_with_gpt(summary, reference, criteria_description=None):\n",
    "    criteria_description = criteria_description or \"\"\"\n",
    "You are an expert summarization evaluator. Evaluate the following summary according to these five criteria from 1 (poor) to 5 (excellent):\n",
    "\n",
    "1. Relevance â€“ Is the summary on-topic and aligned with the reference?\n",
    "2. Clarity â€“ Is the summary easy to read and grammatically correct?\n",
    "3. Conciseness â€“ Is it brief and free of unnecessary information?\n",
    "4. Correctness â€“ Are all factual details (like numbers) accurate?\n",
    "5. Instruction Adherence â€“ Does the summary follow the intended task (e.g., journalistic tone, impact focus)?\n",
    "Return only a dictionary in Python format with these five keys.\n",
    "\"\"\"\n",
    "    full_prompt = f\"{criteria_description}\\n\\nReference:\\n{reference}\\n\\nSummary:\\n{summary}\\n\\nEvaluation:\"\n",
    "    \n",
    "    response = model.generate_content(full_prompt)\n",
    "    \n",
    "    # Parse the returned dictionary string into a Python dict safely\n",
    "    try:\n",
    "        raw = response.text.strip()\n",
    "        \n",
    "        # Rimuove i delimitatori ```python ... ```\n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.strip(\"`\").split(\"python\")[-1].strip()\n",
    "\n",
    "        # Eval sicuro solo su dizionari con chiavi attese\n",
    "        result = eval(raw)\n",
    "        assert all(k in result for k in [\"Relevance\", \"Clarity\", \"Conciseness\", \"Correctness\", \"Instruction Adherence\"])\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ GPT evaluation parsing failed:\", e)\n",
    "        print(\"Raw response:\", response.text)\n",
    "        return {k: 3 for k in [\"Relevance\", \"Clarity\", \"Conciseness\", \"Correctness\", \"Instruction Adherence\"]}\n",
    "\n",
    "\n",
    "# Evaluate both summaries\n",
    "metrics_weak = evaluate_metrics(summary_weak, reference_summary)\n",
    "metrics_strong = evaluate_metrics(summary_strong, reference_summary)\n",
    "\n",
    "\n",
    "human_weak = evaluate_with_gpt(summary_weak, reference_summary)\n",
    "human_strong = evaluate_with_gpt(summary_strong, reference_summary)\n",
    "\n",
    "\n",
    "# Define weighted scoring\n",
    "weights = {\n",
    "    'ROUGE-L': 0.3,\n",
    "    'BLEU': 0.1,\n",
    "    'BERTScore-F1': 0.3,\n",
    "    'Relevance': 0.1,\n",
    "    'Correctness': 0.1,\n",
    "    'Instruction Adherence': 0.1\n",
    "}\n",
    "\n",
    "def compute_weighted(metrics, human):\n",
    "    combined = {**metrics, **human}\n",
    "    return round(sum(combined[k] * w for k, w in weights.items()), 3)\n",
    "\n",
    "score_weak = compute_weighted(metrics_weak, human_weak)\n",
    "score_strong = compute_weighted(metrics_strong, human_strong)\n",
    "\n",
    "\n",
    "\n",
    "# Print output as requested\n",
    "print(\"ðŸ“ Weak Prompt Summary:\")\n",
    "print(summary_weak)\n",
    "print(f\"\\n  ROUGE-L: {metrics_weak['ROUGE-L']}\")\n",
    "print(f\"  BLEU: {metrics_weak['BLEU']}\")\n",
    "print(f\"  BERTScore-F1: {metrics_weak['BERTScore-F1']}\")\n",
    "print(f\"  Length: {metrics_weak['Length']}\\n\")\n",
    "\n",
    "print(\"ðŸ“ Strong Prompt Summary:\")\n",
    "print(summary_strong)\n",
    "print(f\"\\n  ROUGE-L: {metrics_strong['ROUGE-L']}\")\n",
    "print(f\"  BLEU: {metrics_strong['BLEU']}\")\n",
    "print(f\"  BERTScore-F1: {metrics_strong['BERTScore-F1']}\")\n",
    "print(f\"  Length: {metrics_strong['Length']}\\n\")\n",
    "\n",
    "print(f\"ðŸ† Weighted Score (Weak): {score_weak}\")\n",
    "print(f\"ðŸ† Weighted Score (Strong): {score_strong}\\n\")\n",
    "\n",
    "if score_strong > score_weak:\n",
    "    print(\"âœ… Strong prompt performed better across the majority of metrics.\")\n",
    "else:\n",
    "    print(\"âœ… Weak prompt performed better across the majority of metrics.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
