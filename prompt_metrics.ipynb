{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3728ed37",
   "metadata": {},
   "source": [
    "## With reference_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58b4df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Weak Prompt Summary:\n",
      "Okay, based on the text:\n",
      "\n",
      "It describes a new **AI model** developed by **TechLabs researchers**. This model uses **reinforcement learning** to significantly reduce **energy consumption** (specifically a **30% reduction**) in **data centers** by dynamically managing server workloads. The anticipated benefits are **millions in savings** and a **significant reduction in environmental impact**.\n",
      "\n",
      "  ROUGE-L: 0.26\n",
      "  BLEU: 0.01\n",
      "  BERTScore-F1: 0.87\n",
      "  Length: 90\n",
      "\n",
      "ðŸ“ Strong Prompt Summary:\n",
      "Here's a summary:\n",
      "\n",
      "A groundbreaking AI developed by TechLabs can cut energy use in data centers by 30% through smarter server management. This promises massive savings for businesses and a significant positive impact on the environment.\n",
      "\n",
      "  ROUGE-L: 0.19\n",
      "  BLEU: 0.03\n",
      "  BERTScore-F1: 0.9\n",
      "  Length: 40\n",
      "\n",
      "ðŸ† Weighted Score (Weak): 3.42\n",
      "ðŸ† Weighted Score (Strong): 3.916\n",
      "\n",
      "âœ… Strong prompt performed better across the majority of metrics.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bert_score')\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "# Ensure punkt tokenizer is available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "# Load .env and configure Gemini\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash-preview-04-17\")\n",
    "\n",
    "# Input and prompts\n",
    "input_text = \"\"\"\n",
    "A groundbreaking AI model developed by researchers at TechLabs has achieved a 30% reduction in \n",
    "energy consumption in data centers.\n",
    "The model uses reinforcement learning to dynamically manage server workloads, avoiding unnecessary energy usage.\n",
    "This could lead to millions in savings and a significant reduction in environmental impact.\n",
    "\"\"\"\n",
    "reference_summary = (\n",
    "    \"TechLabs' new AI model reduces data center energy use by 30% using reinforcement learning to optimize workloads.\"\n",
    ")\n",
    "\n",
    "prompt_weak = \"Say to me something about the text.\"\n",
    "prompt_strong = (\n",
    "    \"You are a technical journalist. In 1â€“2 sentences, summarize the key innovation and its real-world impact for a general audience.\"\n",
    ")\n",
    "\n",
    "def get_summary(prompt, text):\n",
    "    full_prompt = f\"{prompt}\\n\\n{text}\"\n",
    "    response = model.generate_content(full_prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "summary_weak = get_summary(prompt_weak, input_text)\n",
    "summary_strong = get_summary(prompt_strong, input_text)\n",
    "\n",
    "def evaluate_metrics(summary, reference):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge = scorer.score(reference, summary)['rougeL'].fmeasure\n",
    "    ref_tokens = [word_tokenize(reference, preserve_line=True)]\n",
    "    summary_tokens = word_tokenize(summary, preserve_line=True)\n",
    "    bleu = sentence_bleu(ref_tokens, summary_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "    P, R, F1 = bert_score([summary], [reference], lang=\"en\", verbose=False)\n",
    "    bert_f1 = F1[0].item()\n",
    "    length = len(summary_tokens)\n",
    "    return {\n",
    "        'ROUGE-L': round(rouge, 2),\n",
    "        'BLEU': round(bleu, 2),\n",
    "        'BERTScore-F1': round(bert_f1, 2),\n",
    "        'Length': length\n",
    "    }\n",
    "\n",
    "def evaluate_with_gpt(summary, reference, criteria_description=None):\n",
    "    criteria_description = criteria_description or \"\"\"\n",
    "You are an expert summarization evaluator. Evaluate the following summary according to these five criteria from 1 (poor) to 5 (excellent):\n",
    "\n",
    "1. Relevance â€“ Is the summary on-topic and aligned with the reference?\n",
    "2. Clarity â€“ Is the summary easy to read and grammatically correct?\n",
    "3. Conciseness â€“ Is it brief and free of unnecessary information?\n",
    "4. Correctness â€“ Are all factual details (like numbers) accurate?\n",
    "5. Instruction Adherence â€“ Does the summary follow the intended task (e.g., journalistic tone, impact focus)?\n",
    "Return only a dictionary in Python format with these five keys.\n",
    "\"\"\"\n",
    "    full_prompt = f\"{criteria_description}\\n\\nReference:\\n{reference}\\n\\nSummary:\\n{summary}\\n\\nEvaluation:\"\n",
    "    \n",
    "    response = model.generate_content(full_prompt)\n",
    "    \n",
    "    # Parse the returned dictionary string into a Python dict safely\n",
    "    try:\n",
    "        raw = response.text.strip()\n",
    "        \n",
    "        # Rimuove i delimitatori ```python ... ```\n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.strip(\"`\").split(\"python\")[-1].strip()\n",
    "\n",
    "        # Eval sicuro solo su dizionari con chiavi attese\n",
    "        result = eval(raw)\n",
    "        assert all(k in result for k in [\"Relevance\", \"Clarity\", \"Conciseness\", \"Correctness\", \"Instruction Adherence\"])\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ GPT evaluation parsing failed:\", e)\n",
    "        print(\"Raw response:\", response.text)\n",
    "        return {k: 3 for k in [\"Relevance\", \"Clarity\", \"Conciseness\", \"Correctness\", \"Instruction Adherence\"]}\n",
    "\n",
    "\n",
    "# Evaluate both summaries\n",
    "metrics_weak = evaluate_metrics(summary_weak, reference_summary)\n",
    "metrics_strong = evaluate_metrics(summary_strong, reference_summary)\n",
    "\n",
    "\n",
    "human_weak = evaluate_with_gpt(summary_weak, reference_summary)\n",
    "human_strong = evaluate_with_gpt(summary_strong, reference_summary)\n",
    "\n",
    "\n",
    "# Define weighted scoring\n",
    "weights = {\n",
    "    # automatic\n",
    "    \"ROUGE-L\":      0.10,\n",
    "    \"BLEU\":         0.05,\n",
    "    \"BERTScore-F1\": 0.05,\n",
    "    # LLM-as-judge  â† keys must match exactly what evaluate_with_gpt returns\n",
    "    \"Relevance\":            0.30,\n",
    "    \"Clarity\":              0.15,\n",
    "    \"Conciseness\":          0.15,\n",
    "    \"Correctness\":          0.10,\n",
    "    \"Instruction Adherence\":0.10,\n",
    "}\n",
    "\n",
    "\n",
    "def compute_weighted(metrics, human):\n",
    "    combined = {**metrics, **human}\n",
    "    return round(sum(combined[k] * w for k, w in weights.items()), 3)\n",
    "\n",
    "score_weak = compute_weighted(metrics_weak, human_weak)\n",
    "score_strong = compute_weighted(metrics_strong, human_strong)\n",
    "\n",
    "\n",
    "\n",
    "# Print output as requested\n",
    "print(\"ðŸ“ Weak Prompt Summary:\")\n",
    "print(summary_weak)\n",
    "print(f\"\\n  ROUGE-L: {metrics_weak['ROUGE-L']}\")\n",
    "print(f\"  BLEU: {metrics_weak['BLEU']}\")\n",
    "print(f\"  BERTScore-F1: {metrics_weak['BERTScore-F1']}\")\n",
    "print(f\"  Length: {metrics_weak['Length']}\\n\")\n",
    "\n",
    "print(\"ðŸ“ Strong Prompt Summary:\")\n",
    "print(summary_strong)\n",
    "print(f\"\\n  ROUGE-L: {metrics_strong['ROUGE-L']}\")\n",
    "print(f\"  BLEU: {metrics_strong['BLEU']}\")\n",
    "print(f\"  BERTScore-F1: {metrics_strong['BERTScore-F1']}\")\n",
    "print(f\"  Length: {metrics_strong['Length']}\\n\")\n",
    "\n",
    "print(f\"ðŸ† Weighted Score (Weak): {score_weak}\")\n",
    "print(f\"ðŸ† Weighted Score (Strong): {score_strong}\\n\")\n",
    "\n",
    "if score_strong > score_weak:\n",
    "    print(\"âœ… Strong prompt performed better across the majority of metrics.\")\n",
    "else:\n",
    "    print(\"âœ… Weak prompt performed better across the majority of metrics.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525aa5a",
   "metadata": {},
   "source": [
    "## Without reference_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf08813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WEAK SUMMARY ===\n",
      "Okay, here's something about the text:\n",
      "\n",
      "The text describes a **groundbreaking AI model** developed by **TechLabs researchers**. Its main achievement is significantly reducing **energy consumption in data centers** by **30%** using **reinforcement learning**. This innovation is expected to bring about **large financial savings** and a **major positive environmental impact**.\n",
      "â€’-â€’- Automatic metrics\n",
      "  ROUGE-L     : 0.45\n",
      "  BLEU        : 0.06\n",
      "  BERTScore-F1: 0.87\n",
      "  Length      : 84\n",
      "â€’-â€’- Gemini rubric\n",
      "  Faithfulness: 5\n",
      "  Coverage    : 5\n",
      "  Clarity     : 4\n",
      "  Conciseness : 4\n",
      "  Fluency     : 4\n",
      "\n",
      "=== STRONG SUMMARY ===\n",
      "Researchers have developed an AI that achieves a 30% reduction in data center energy consumption by intelligently managing server workloads. This breakthrough promises substantial cost savings and a significant positive impact on the environment.\n",
      "â€’-â€’- Automatic metrics\n",
      "  ROUGE-L     : 0.35\n",
      "  BLEU        : 0.13\n",
      "  BERTScore-F1: 0.91\n",
      "  Length      : 36\n",
      "â€’-â€’- Gemini rubric\n",
      "  Faithfulness: 5\n",
      "  Coverage    : 5\n",
      "  Clarity     : 5\n",
      "  Conciseness : 4\n",
      "  Fluency     : 5\n",
      "\n",
      "ðŸ† Weighted Score (Weak)  : 3.692\n",
      "ðŸ† Weighted Score (Strong): 3.937\n",
      "\n",
      "âœ… The *strong* prompt performed better across the majority of metrics.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Imports & setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, warnings, nltk\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai          # Gemini SDK\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score as bert_score   # For semantic similarity\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bert_score\")\n",
    "\n",
    "# Make sure NLTKâ€™s punkt tokenizer is available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "# Load Gemini API key & choose a model\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash-preview-04-17\")\n",
    "\n",
    "# â”€â”€ Source document & prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "source_text = \"\"\"\n",
    "A groundbreaking AI model developed by researchers at TechLabs has achieved a 30 % reduction in \n",
    "energy consumption in data centers. The model uses reinforcement learning to dynamically manage\n",
    "server workloads, avoiding unnecessary energy usage. This breakthrough could lead to millions of\n",
    "dollars in savings and a significant reduction in environmental impact.\n",
    "\"\"\"\n",
    "\n",
    "prompt_weak   = \"Say to me something about the text.\"\n",
    "prompt_strong = (\n",
    "    \"You are a technical journalist. In 1-2 sentences, summarize the key innovation and its real-\"\n",
    "    \"world impact for a general audience.\"\n",
    ")\n",
    "\n",
    "# â”€â”€ Summarisation helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_summary(prompt: str, text: str) -> str:\n",
    "    \"\"\"Generate a summary of *text* according to *prompt* using Gemini.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n\\n{text}\"\n",
    "    response    = model.generate_content(full_prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "summary_weak   = get_summary(prompt_weak,   source_text)\n",
    "summary_strong = get_summary(prompt_strong, source_text)\n",
    "\n",
    "# â”€â”€ Automatic metrics (reference-free) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_metrics(summary: str, source: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compute overlap-style metrics *against the source document* (no gold summary needed).\n",
    "    ROUGE-L recall, BLEU precision, BERTScore-F1, and length.\n",
    "    \"\"\"\n",
    "    scorer     = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    rouge_l    = scorer.score(source, summary)[\"rougeL\"].recall         # coverage of source\n",
    "    source_tok = [word_tokenize(source, preserve_line=True)]\n",
    "    summ_tok   = word_tokenize(summary, preserve_line=True)\n",
    "    bleu       = sentence_bleu(source_tok, summ_tok,\n",
    "                               smoothing_function=SmoothingFunction().method1)\n",
    "    _, _, F1   = bert_score([summary], [source], lang=\"en\", verbose=False)\n",
    "    return {\n",
    "        \"ROUGE-L\":      round(rouge_l,  2),\n",
    "        \"BLEU\":         round(bleu,     2),\n",
    "        \"BERTScore-F1\": round(F1[0].item(), 2),\n",
    "        \"Length\":       len(summ_tok),\n",
    "    }\n",
    "\n",
    "# â”€â”€ LLM-as-Judge (no reference) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_with_gpt_no_ref(summary: str, source_doc: str,\n",
    "                             criteria_description: str | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Ask Gemini to rate a summary **without any gold reference**.\n",
    "    The LLM sees the SOURCE and the SUMMARY and returns scores 1-5 for each rubric item.\n",
    "    \"\"\"\n",
    "    criteria_description = criteria_description or \"\"\"\n",
    "You are an expert summarisation evaluator. Judge the SUMMARY only against the SOURCE according to\n",
    "these five criteria, each on a 1-5 scale (1 = poor, 5 = excellent):\n",
    "1. Faithfulness â€“ No hallucinations; every claim is supported by the source.\n",
    "2. Coverage     â€“ Captures the main, important points of the source.\n",
    "3. Clarity      â€“ Easy to read; well structured; no jargon unless explained.\n",
    "4. Conciseness  â€“ Succinct yet complete.\n",
    "5. Fluency      â€“ Grammatically correct and natural English.\n",
    "\n",
    "Return a valid Python dict exactly like:\n",
    "{\"Faithfulness\": 4, \"Coverage\": 5, \"Clarity\": 5, \"Conciseness\": 4, \"Fluency\": 5}\n",
    "\"\"\"\n",
    "    prompt = f\"{criteria_description.strip()}\\n\\nSOURCE:\\n{source_doc}\\n\\nSUMMARY:\\n{summary}\\n\\nEVALUATION:\"\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    # Parse the dict safely\n",
    "    try:\n",
    "        raw = response.text.strip()\n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.strip(\"`\").split(\"python\")[-1].strip()\n",
    "        result = eval(raw)                                        # <-- trusted parsing for demo\n",
    "        expected = {\"Faithfulness\", \"Coverage\", \"Clarity\", \"Conciseness\", \"Fluency\"}\n",
    "        assert expected.issubset(result)\n",
    "        return result\n",
    "    except Exception as err:\n",
    "        print(\"âš ï¸  Gemini evaluation parsing failed:\", err)\n",
    "        print(\"Raw response:\\n\", response.text)\n",
    "        # Fall back to neutral (3) for each criterion\n",
    "        return {k: 3 for k in [\"Faithfulness\", \"Coverage\", \"Clarity\", \"Conciseness\", \"Fluency\"]}\n",
    "\n",
    "# â”€â”€ Gather scores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "metrics_weak   = evaluate_metrics(summary_weak,   source_text)\n",
    "metrics_strong = evaluate_metrics(summary_strong, source_text)\n",
    "\n",
    "human_weak     = evaluate_with_gpt_no_ref(summary_weak,   source_text)\n",
    "human_strong   = evaluate_with_gpt_no_ref(summary_strong, source_text)\n",
    "\n",
    "# Combine all scores\n",
    "def weighted_score(auto: dict, human: dict, weights: dict) -> float:\n",
    "    total = 0\n",
    "    for k, w in weights.items():\n",
    "        total += w * (auto.get(k) or human.get(k))\n",
    "    return round(total, 3)\n",
    "\n",
    "weights = {\n",
    "    # automatic\n",
    "    \"ROUGE-L\":      0.10,\n",
    "    \"BLEU\":         0.05,\n",
    "    \"BERTScore-F1\": 0.05,\n",
    "    # LLM-as-judge\n",
    "    \"Faithfulness\": 0.30,\n",
    "    \"Coverage\":     0.10,\n",
    "    \"Clarity\":      0.15,\n",
    "    \"Conciseness\":  0.15,\n",
    "    \"Fluency\":      0.10,\n",
    "}\n",
    "\n",
    "score_weak   = weighted_score(metrics_weak,   human_weak,   weights)\n",
    "score_strong = weighted_score(metrics_strong, human_strong, weights)\n",
    "\n",
    "# â”€â”€ Pretty print results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def report(name, summary, auto, human):\n",
    "    print(f\"\\n=== {name} SUMMARY ===\")\n",
    "    print(summary)\n",
    "    print(\"â€’-â€’- Automatic metrics\")\n",
    "    for m, v in auto.items():   print(f\"  {m:12}: {v}\")\n",
    "    print(\"â€’-â€’- Gemini rubric\")\n",
    "    for m, v in human.items():  print(f\"  {m:12}: {v}\")\n",
    "\n",
    "report(\"WEAK\",   summary_weak,   metrics_weak,   human_weak)\n",
    "report(\"STRONG\", summary_strong, metrics_strong, human_strong)\n",
    "\n",
    "print(f\"\\nðŸ† Weighted Score (Weak)  : {score_weak}\")\n",
    "print(f\"ðŸ† Weighted Score (Strong): {score_strong}\\n\")\n",
    "\n",
    "if score_strong > score_weak:\n",
    "    print(\"âœ… The *strong* prompt performed better across the majority of metrics.\")\n",
    "elif score_strong < score_weak:\n",
    "    print(\"âœ… The *weak* prompt performed better across the majority of metrics.\")\n",
    "else:\n",
    "    print(\"ðŸ¤ The two prompts are tied under the current weighting scheme.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60c029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
